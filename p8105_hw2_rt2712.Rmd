---
title: "p8105_hw2_rt2712"
author: "Rachel Tsong"
date: "5 October 2018"
output: github_document
---

# Problem 1

## Step 1

Load and clean up data
```{r}
library(tidyverse)

subway_data = (
  read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names() %>%
  select(line:entry, vending, ada) %>%
  mutate(entry = recode(entry, "YES" = TRUE, "NO" = FALSE))
)
```

### Summary of dataset

First I imported the data from the .csv file and cleaned the names so that the dataset is easier to work with. After that, I selected the columns for line, station name, station latitude and longitude, entrance type, routes served, vending, and ADA compliance. Lastly, I changed the entry column into a logical vector. The resulting data set is 1868 x 19. These data give information about subway stations and their entrances/exits including whether or not they have ticket vending machines, whether or not they are accessible, location, and routes served. Right now, the data frame is not yet tidy because there are variables in the column names: route1-route11. These columns need to be reformatted so that each variable has its own column, each observation has its own row, and there is a single value in every cell.

## Step 2 

### Answer questions:

**How many distinct stations?**
```{r}
distinct(subway_data, line, station_name)
```

There are 465 unique stations. 

**How many stations are ada compliant?**
```{r}
filter(
  (distinct(subway_data, line, station_name, .keep_all = TRUE)),
  ada == TRUE)
```

84 distinct stations are ADA compliant.

**What proportion of entrances/exits without vending allow entrance?**
```{r}
filter(subway_data, vending == "NO")

filter(subway_data, vending == "NO", entry == TRUE)
```

I ran the first line of the code chunk above to tell me how many stations don't have vending and the second line to tell me how many stations that do not offer vending allow entrance. I can conclude from the resulting tibbles that 69/183 or 37.7% of stations that do not offer vending do allow entrance.

## Step 3

Reformat route # and route name, answer questions
```{r}
subway_data_tidy = gather(subway_data, key = route_number, value = route, route1:route11)

filter(
  (distinct(subway_data_tidy, line, station_name, .keep_all = TRUE)),
  route == "A"
)

filter(
  (distinct(subway_data_tidy, line, station_name, .keep_all = TRUE)),
  route == "A", ada == TRUE
)
```

There are 60 distinct subway stations that serve the A train, and 17 of these stations that serve the A train are ADA compliant.

# Problem 2

## Step 1

Load and clean up data
```{r read and clean sheet 1}
library(readxl)

trash_wheel = (
  read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", sheet = 1, range = "A2:N336") %>%
  janitor::clean_names() %>%
  drop_na(dumpster) %>% 
  mutate(sports_balls = as.integer(round(sports_balls)))
)
```

```{r read and clean precip data}
precip_2017 = (
  read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", sheet = 4, range = "A2:B14") %>% 
  janitor::clean_names() %>%
  mutate(year = "2017")
)

precip_2016 = (
  read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", sheet = 5, range = "A2:B14") %>% 
  janitor::clean_names() %>%
  mutate(year = "2016")
)

precip_data = (
  bind_rows(precip_2016, precip_2017) %>%
  mutate(month = month.name[month]) %>%
  rename(total_inches = total)
)

```

## Step 2

**Description of Data**

In the trash_wheel data set, there are `r nrow(trash_wheel)` observations. These data show the amount of trash that Mr. Trash Wheel picked up from the Jones Falls River in Baltimore from May 2014 until July 2018. The columns in the data set show the types of trash  collected (e.g. cigarette butts, glass bottles, etc.) as well as temporal information. Additionally, the data give the number of homes powered by incineration of the trash collected. I checked over these column names and thought they were named appropriately. I considered changing the variable names for weight and volume, since these were the only column headings which included units and I wanted consistency; however, I decided that the units were essential to understanding the values. Each row represents the contents of a single full "dumpster," which is what Mr. Trash Wheel deposits the trash collected into. The median number of sports balls collected per dumpster in 2016 is `r median(trash_wheel$sports_balls[trash_wheel$year == "2016"])` balls.

The precip_data data set gives the total amount of precipitation per month for 2016 and 2017. In the original data, the units for the precipitation total per month was given in the title. I decided to provide that information in the column name for totals, so I renamed that column total_inches. I thought this was important information to retain to prevent ambiguity. The total amount of precipitation in 2017 was `r sum(precip_2017$total)` inches.


# Problem 3

## Step 1

Load and clean up data
```{r}
devtools::install_github("p8105/p8105_datasets")

library(p8105.datasets)

data(brfss_smart2010)

brfss_data = (
  janitor::clean_names(brfss_smart2010) %>%
  filter(topic == "Overall Health") %>%
  select(year:locationdesc, response, data_value) %>%
  rename(state = locationabbr, county = locationdesc) %>%
  spread(key = response, value = data_value) %>%
  janitor::clean_names() %>%
  mutate(upper_rating = excellent + very_good) %>%
  select(year:excellent, very_good, good, fair, poor, upper_rating)
)
```

In the code above, I loaded the data set and selected the rows for overall health and the columns for the relevent information. After that, I noticed that the variable names for location were not very informative, so I changed them to "state" and "county" to be more clear what information was in each column. Then I made the response to the question "How is your general health?" a column and rows in this column indicate the proportion of responders who gave each rating at a particular location. Last, I made a variable for "upper rating" to indicate the proportion of responders who answered "excellent" or "very good" and then re-ordered the columns in a logical way.

## Step 2

### Answer questions

**Locations**

There are `r nrow(distinct(brfss_data, county))` different counties in the dataset, each representing a unique location. There are `r nrow(distinct(brfss_data, state))` distinct entries in the "state" category, which at first I though was a mistake, but upon closer inspection I discovered that Washington, D.C. had its own row and all 50 states are represented as well. Using the summary table obtained by running ```summary(as.factor(brfss_data$state))``` it can be determined that New Jersey has the most observations (146).

**2002 Excellent Responses**

In 2002, the median proportion of "excellent" responses was `r median(brfss_data$excellent[brfss_data$year == "2002"], na.rm = TRUE)`. 

**Plots**

```{r histogram}
library(ggplot2)

brfss_histo = filter(brfss_data, year == 2002)

ggplot(brfss_histo, aes(x = excellent)) + 
  geom_histogram()
```

```{r scatterplot}
brfss_scatter = (
  filter(brfss_data, county == "NY - New York County" | county == "NY - Queens County")
)

ggplot(brfss_scatter, aes(x = year, y = excellent, color = county)) + 
  geom_point()
```









